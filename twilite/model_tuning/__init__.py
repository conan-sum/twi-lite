class Model:
    """
    Parameters
    ----------
    model: clustering model
        The clustering model used to cluster the embeddings

    eval_range: list()
        The range of hyperparamters to be evaluated

    Attributes
    ----------
    best_result: float
        The score for the best hyperparameter within the range

    best_model: clustering model
        The clustering model that returned the highest score

    """

    def __init__(self, model, eval_range):
        self.model = model
        self.eval_range = eval_range
        self.best_result = None
        self.best_model = None

    def __repr__(self):
        model_name = self.model().__class__.__name__
        return f'{model_name}'

    def evaluate(self, df, metric):
        S = []
        arr = df[['xcord', 'ycord']].to_numpy()
        for i in self.eval_range:
            labels = self.model(i).fit_predict(arr)
            score = metric(arr, labels)
            S.append((self.model(i), score))
        S.sort(key=lambda x: x[1], reverse=True)
        self.best_model = S[0][0]
        self.best_result = S[0][1]
        return None


class GridSearch:
    """
    Parameters
    ----------
    models: list()
        A list of class Model objects

    metric: score function
        A function that calculates the quality of the cluster generated by the model and parameter

    Attributes
    ----------
    best_model: sklearn clustering model
        The clustering model configuration that generates the best cluster

    best_result: float
        The score for the cluster generated with the best model

    labels: pandas dataframe
        A pandas dataframe with the labels

    """
    def __init__(self, models, metric):
        self.models = models
        self.metric = metric
        self.results = []
        self.best_model = None
        self.best_result = None
        self.labels = None

    def __repr__(self):
        metric_name = self.metric.__name__
        if self.best_model:
            model_name = self.best_model.__class__.__name__
            return f'GridSearch(best_model={model_name}, metric={metric_name}, score={self.best_result})'
        return f'GridSearch(models={self.models}, metric={metric_name})'

    def search(self, df):
        for i in self.models:
            i.evaluate(df, self.metric)
            self.results.append((i.best_model, i.best_result))
        self.results.sort(key=lambda x: x[1], reverse=True)
        self.best_model = self.results[0][0]
        self.best_result = self.results[0][1]
        arr = df[['xcord', 'ycord']].to_numpy()
        self.labels = self.best_model.fit_predict(arr)
        return None

